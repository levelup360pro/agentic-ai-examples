{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07083d05",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "  <a href=\"https://www.linkedin.com/company/100622063\" target=\"_blank\" title=\"Follow LevelUp360 on LinkedIn\">\n",
    "    <img src=\"../../assets/levelup360-inverted-logo-transparent.svg\" alt=\"LevelUp360\" width=\"220\">\n",
    "  </a>\n",
    "</p>\n",
    "\n",
    "\n",
    "# Marketing Team – Week 6: content planning agent Routing Tests\n",
    "\n",
    "**Objective:** Validate that the Microsoft Agent Framework content planning agent correctly routes queries to appropriate tools before building the full workflow.\n",
    "\n",
    "**Pass Criteria:**\n",
    "- Routing accuracy: >90%\n",
    "- Routing consistency: >95%\n",
    "\n",
    "**Test Approach:**\n",
    "1. Define test cases with known correct routing decisions\n",
    "2. Measure routing accuracy across all test cases\n",
    "3. Measure routing consistency (5 runs per case)\n",
    "4. Analyze failures and inconsistencies\n",
    "5. Pass/fail decision based on thresholds\n",
    "\n",
    "**NOTE: Accuracy and consistency results are dependent on the system message you use in brand config YAML (i.e. itconsulting.yaml) models/content_planning/system_message**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80343a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and Setup (Week 6 – Microsoft Agent Framework)\n",
    "import logging\n",
    "from rich import print as rprint\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(name)s | %(message)s')\n",
    "logger = logging.getLogger(\"week6_routing_tests\")\n",
    "logger.info(\"Environment & logging initialized for Week 6 routing tests\")\n",
    "\n",
    "# Framework imports (handler-based executors)\n",
    "from src.core.evaluation.routing_evaluator import RoutingEvaluator, AgentFrameworkRoutingAdapter\n",
    "from src.orchestration.microsoft_agent_framework.workflows.content_generation_workflow import build_content_generation_workflow\n",
    "from src.core.utils.config_loader import load_brand_config\n",
    "\n",
    "# Configuration\n",
    "brand = \"itconsulting\"\n",
    "brand_config = load_brand_config(brand)\n",
    "\n",
    "# Build the Microsoft Agent Framework workflow (executors expose @handler methods)\n",
    "workflow = build_content_generation_workflow(brand=brand)\n",
    "\n",
    "# Initialize adapter and evaluator for Agent Framework\n",
    "adapter = AgentFrameworkRoutingAdapter(\n",
    "    workflow=workflow,\n",
    "    brand=brand,\n",
    "    template=\"LINKEDIN_POST_ZERO_SHOT\",\n",
    "    use_cot=True,\n",
    "    brand_config=brand_config,\n",
    ")\n",
    "\n",
    "evaluator = RoutingEvaluator(adapter)\n",
    "\n",
    "rprint(\"✓ Setup complete (Week 6 – Agent Framework)\")\n",
    "rprint(f\"  Brand: {brand}\")\n",
    "rprint(f\"  Workflow: Microsoft Agent Framework content generation workflow compiled\")\n",
    "rprint(f\"  Tools: rag_search, web_search\")\n",
    "rprint(f\"  Adapter: AgentFrameworkRoutingAdapter\")\n",
    "rprint(f\"  Evaluator: RoutingEvaluator\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efb07763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Test Cases \n",
    "routing_test_cases = [\n",
    "    # Company-specific queries - should use rag_search\n",
    "    {\n",
    "        \"query\": \"Create a post about our AI governance pipeline\",\n",
    "        \"expected_tools\": [\"rag_search\"],  \n",
    "        \"reason\": \"Company-specific implementation details\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Explain how our agentic marketing team works\",\n",
    "        \"expected_tools\": [\"rag_search\"],  \n",
    "        \"reason\": \"Specific technical implementation from our docs\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What services does LevelUp360 offer?\",\n",
    "        \"expected_tools\": [\"rag_search\"],  \n",
    "        \"reason\": \"Company-specific service information\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Describe our Azure cloud architecture approach\",\n",
    "        \"expected_tools\": [\"rag_search\"],  \n",
    "        \"reason\": \"Our specific architecture patterns\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What certifications and expertise do we have?\",\n",
    "        \"expected_tools\": [\"rag_search\"],  \n",
    "        \"reason\": \"Company-specific credentials\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"How do we implement secure AI infrastructure?\",\n",
    "        \"expected_tools\": [\"rag_search\"],  \n",
    "        \"reason\": \"Our specific security implementation\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What is our approach to AI evaluation and governance?\",\n",
    "        \"expected_tools\": [\"rag_search\"],  \n",
    "        \"reason\": \"Company-specific methodology\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Explain our content generation workflow\",\n",
    "        \"expected_tools\": [\"rag_search\"],  \n",
    "        \"reason\": \"Our specific workflow implementation\"\n",
    "    },\n",
    "    \n",
    "    # General knowledge - should use direct_generation (no tools)\n",
    "    {\n",
    "        \"query\": \"Write about the importance of AI governance in general\",\n",
    "        \"expected_tools\": [],\n",
    "        \"reason\": \"General AI governance concepts, no company context needed\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Discuss general cloud security best practices\",\n",
    "        \"expected_tools\": [],\n",
    "        \"reason\": \"General industry knowledge\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Explain what agentic AI systems are\",\n",
    "        \"expected_tools\": [],\n",
    "        \"reason\": \"General AI concept definition\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Write about the benefits of infrastructure as code\",\n",
    "        \"expected_tools\": [],\n",
    "        \"reason\": \"General DevOps concept\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Discuss the role of AI in business transformation\",\n",
    "        \"expected_tools\": [],\n",
    "        \"reason\": \"General business/AI topic\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Explain the concept of retrieval-augmented generation\",\n",
    "        \"expected_tools\": [],\n",
    "        \"reason\": \"General AI architecture concept\"\n",
    "    },\n",
    "    \n",
    "    # Current/industry information - should use web_search\n",
    "    {\n",
    "        \"query\": \"What are the latest trends in AI implementation for 2025?\",\n",
    "        \"expected_tools\": [\"web_search\"],\n",
    "        \"reason\": \"Current industry trends, needs recent information\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are current cloud security compliance requirements?\",\n",
    "        \"expected_tools\": [\"web_search\"],\n",
    "        \"reason\": \"Current regulatory information\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are the latest Azure AI services announcements?\",\n",
    "        \"expected_tools\": [\"web_search\"],\n",
    "        \"reason\": \"Current product updates\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What are companies saying about AI ROI in 2025?\",\n",
    "        \"expected_tools\": [\"web_search\"],\n",
    "        \"reason\": \"Current market sentiment and data\"\n",
    "    },\n",
    "    \n",
    "    # Edge cases - mixed signals\n",
    "    {\n",
    "    \"query\": \"Compare our Azure governance approach to current industry standards\",\n",
    "    \"expected_tools\": [\"rag_search\", \"web_search\"],  \n",
    "    \"reason\": \"Needs our approach (RAG) + industry standards (web search)\"\n",
    "    },\n",
    "    {\n",
    "    \"query\": \"How does our agentic system compare to general AI agent frameworks?\",\n",
    "    \"expected_tools\": [\"rag_search\", \"web_search\"],  \n",
    "    \"reason\": \"Needs our system (RAG) + general frameworks (web search)\"\n",
    "    },\n",
    "]\n",
    "\n",
    "rprint(f'Defined {len(routing_test_cases)} routing test cases.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd9fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the updated adapter module is reloaded in this kernel (no restart needed)\n",
    "import importlib\n",
    "import src.core.evaluation.routing_evaluator as routing_evaluator\n",
    "importlib.reload(routing_evaluator)\n",
    "\n",
    "# Rebind the Adapter class and recreate adapter instance to pick up code changes\n",
    "AgentFrameworkRoutingAdapter = routing_evaluator.AgentFrameworkRoutingAdapter\n",
    "adapter = AgentFrameworkRoutingAdapter(\n",
    "    workflow=workflow,\n",
    "    brand=brand,\n",
    "    template=\"LINKEDIN_POST_ZERO_SHOT\",\n",
    "    use_cot=True,\n",
    "    brand_config=brand_config,\n",
    ")\n",
    "\n",
    "# Run the routing accuracy test using the evaluator helper (returns a DataFrame)\n",
    "accuracy_results = evaluator.test_routing_accuracy(routing_test_cases)\n",
    "# Ensure we have a DataFrame (some helpers may return lists/dicts)\n",
    "if not isinstance(accuracy_results, pd.DataFrame):\n",
    "    accuracy_results = pd.DataFrame(accuracy_results)\n",
    "# Ensure the 'correct' column exists (evaluator should provide it, but recompute to be safe)\n",
    "accuracy_results['correct'] = accuracy_results.apply(\n",
    "    lambda row: set(row['expected_tools']) <= set(row['actual_tools'] if isinstance(row.get('actual_tools'), (list, tuple)) else [row.get('actual_tools')]) ,\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Calculate overall accuracy\n",
    "accuracy = accuracy_results['correct'].mean()\n",
    "\n",
    "rprint(f\"\\n{'=' * 60}\")\n",
    "rprint(f\"ROUTING ACCURACY: {accuracy * 100:.1f}%\")\n",
    "rprint(f\"{'=' * 60}\")\n",
    "rprint(f\"Correct: {accuracy_results['correct'].sum()}/{len(accuracy_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20496cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall accuracy\n",
    "accuracy = accuracy_results['correct'].mean()\n",
    "\n",
    "rprint(f\"\\n{'=' * 60}\")\n",
    "rprint(f\"ROUTING ACCURACY: {accuracy * 100:.1f}%\")\n",
    "rprint(f\"{'=' * 60}\")\n",
    "rprint(f\"Correct: {accuracy_results['correct'].sum()}/{len(accuracy_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70949856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Breakdown by Tool Type\n",
    "rprint(\"\\nAccuracy breakdown:\")\n",
    "rprint(\"-\" * 60)\n",
    "\n",
    "# Helper to categorize test cases\n",
    "def categorize_case(expected_tools):\n",
    "    if len(expected_tools) == 1:\n",
    "        return f\"Single: {expected_tools[0]}\"\n",
    "    else:\n",
    "        return \"Multi-tool\"\n",
    "\n",
    "accuracy_results['category'] = accuracy_results['expected_tools'].apply(\n",
    "    lambda x: categorize_case(x)\n",
    ")\n",
    "\n",
    "by_category = accuracy_results.groupby('category').agg({\n",
    "    'correct': ['sum', 'count', 'mean']\n",
    "}).round(3)\n",
    "\n",
    "by_category.columns = ['Correct', 'Total', 'Accuracy']\n",
    "by_category['Accuracy %'] = (by_category['Accuracy'] * 100).round(1)\n",
    "\n",
    "rprint(by_category[['Correct', 'Total', 'Accuracy %']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414bf66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Show Routing Failures\n",
    "failures = accuracy_results[~accuracy_results['correct']]\n",
    "\n",
    "if len(failures) > 0:\n",
    "    rprint(f\"\\n{'=' * 60}\")\n",
    "    rprint(f\"X ROUTING FAILURES: {len(failures)}\")\n",
    "    rprint(f\"{'=' * 60}\\n\")\n",
    "    \n",
    "    for idx, row in failures.iterrows():\n",
    "        rprint(f\"Query: {row['query']}\")\n",
    "        rprint(f\"  Expected: {row['expected_tools']}\")\n",
    "        rprint(f\"  Actual: {row['actual_tools']}\")\n",
    "        rprint(f\"  content planning agent reasoning: {row['reasoning']}\")\n",
    "        rprint(f\"  Why expected: {row['reason_for_expected']}\")\n",
    "        rprint()\n",
    "else:\n",
    "    rprint(f\"\\n{'=' * 60}\")\n",
    "    rprint(\"✓ NO ROUTING FAILURES\")\n",
    "    rprint(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5ccfae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test 2: Routing Consistency\n",
    "\n",
    "Testing if the content planning agent makes the same routing decision across multiple runs.\n",
    "\n",
    "**Metric:** Percentage of test cases where all 5 runs produce the same routing decision\n",
    "\n",
    "**Target:** >95% consistency\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89431011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Routing Consistency Tests\n",
    "rprint(\"Running routing consistency tests (5 runs per case)...\")\n",
    "rprint(\"=\" * 60)\n",
    "\n",
    "consistency_results = evaluator.test_routing_consistency(\n",
    "    routing_test_cases,\n",
    "    num_runs=3\n",
    ")\n",
    "\n",
    "# Calculate overall consistency\n",
    "consistency = consistency_results['consistent'].mean()\n",
    "\n",
    "rprint(f\"\\n{'=' * 60}\")\n",
    "rprint(f\"ROUTING CONSISTENCY: {consistency * 100:.1f}%\")\n",
    "rprint(f\"{'=' * 60}\")\n",
    "rprint(f\"Consistent: {consistency_results['consistent'].sum()}/{len(consistency_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520ad2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Inconsistent Routing Cases\n",
    "inconsistent = consistency_results[~consistency_results['consistent']]\n",
    "\n",
    "if len(inconsistent) > 0:\n",
    "    rprint(f\"\\n{'=' * 60}\")\n",
    "    rprint(f\"  INCONSISTENT ROUTING: {len(inconsistent)}\")\n",
    "    rprint(f\"{'=' * 60}\\n\")\n",
    "    \n",
    "    for idx, row in inconsistent.iterrows():\n",
    "        rprint(f\"Query: {row['query']}\")\n",
    "        rprint(f\"  Expected: {row['expected_tools']}\")\n",
    "        rprint(f\"  Decisions across 5 runs:\")\n",
    "        for i, decision in enumerate(row['decisions'], 1):\n",
    "            rprint(f\"    Run {i}: {decision}\")\n",
    "        rprint(f\"  Unique decision patterns: {row['variance']}\")\n",
    "        rprint()\n",
    "else:\n",
    "    rprint(f\"\\n{'=' * 60}\")\n",
    "    rprint(\"✓ ALL ROUTING DECISIONS CONSISTENT\")\n",
    "    rprint(f\"{'=' * 60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c966b8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis\n",
    "analysis = evaluator.analyze_results(\n",
    "    accuracy_results,\n",
    "    consistency_results,\n",
    "    accuracy_threshold=0.90,\n",
    "    consistency_threshold=0.95\n",
    ")\n",
    "\n",
    "rprint(\"=\" * 60)\n",
    "rprint(\"ANALYSIS\")\n",
    "rprint(\"=\" * 60)\n",
    "rprint(f\"\\nAccuracy: {analysis['accuracy'] * 100:.1f}% (threshold: {analysis['thresholds']['accuracy'] * 100:.0f}%)\")\n",
    "rprint(f\"Consistency: {analysis['consistency'] * 100:.1f}% (threshold: {analysis['thresholds']['consistency'] * 100:.0f}%)\")\n",
    "\n",
    "rprint(f\"\\nFailure summary:\")\n",
    "rprint(f\"  Routing failures: {len(analysis['failures'])}\")\n",
    "rprint(f\"  Inconsistent cases: {len(analysis['inconsistent_cases'])}\")\n",
    "\n",
    "if len(analysis['failures']) > 0:\n",
    "    rprint(f\"\\nMost common failure patterns:\")\n",
    "    failure_df = pd.DataFrame(analysis['failures'])\n",
    "    # Show which expected tools had most failures\n",
    "    if 'expected_tools' in failure_df.columns:\n",
    "        rprint(failure_df['expected_tools'].value_counts().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602d1141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Decision\n",
    "rprint(\"\\n\" + \"=\" * 60)\n",
    "rprint(\"FINAL ASSESSMENT\")\n",
    "rprint(\"=\" * 60)\n",
    "\n",
    "if analysis['passes']:\n",
    "    rprint(\"\\n✓ PASS: content planning agent routing is reliable\")\n",
    "    rprint(f\"\\n  Accuracy: {analysis['accuracy'] * 100:.1f}% ✓\")\n",
    "    rprint(f\"  Consistency: {analysis['consistency'] * 100:.1f}% ✓\")\n",
    "    rprint(\"\\nNext steps:\")\n",
    "    rprint(\"  → Proceed to building full Microsoft Agent Framework workflow\")\n",
    "    rprint(\"  → Test end-to-end content quality vs Weeks 3 baseline\")\n",
    "    rprint(\"  → Validate multi-tool coordination in full workflow\")\n",
    "else:\n",
    "    rprint(\"\\nX FAIL: content planning agent routing needs tuning\")\n",
    "    rprint(\"\\nIssues identified:\")\n",
    "    \n",
    "    if analysis['accuracy'] <= 0.90:\n",
    "        rprint(f\"  → Accuracy too low: {analysis['accuracy'] * 100:.1f}% (target: >90%)\")\n",
    "        rprint(\"     Action: Review failed cases and adjust content planning agent prompt\")\n",
    "        \n",
    "        # Show specific failure patterns\n",
    "        if len(analysis['failures']) > 0:\n",
    "            rprint(\"\\n     Common failure types:\")\n",
    "            failure_df = pd.DataFrame(analysis['failures'])\n",
    "            for _, failure in failure_df.head(3).iterrows():\n",
    "                rprint(f\"       - Expected {failure['expected_tools']}, got {failure['actual_tools']}\")\n",
    "    \n",
    "    if analysis['consistency'] <= 0.95:\n",
    "        rprint(f\"  → Consistency too low: {analysis['consistency'] * 100:.1f}% (target: >95%)\")\n",
    "        rprint(\"     Action: Consider lowering temperature or adjusting routing logic\")\n",
    "    \n",
    "    rprint(\"\\nNext steps:\")\n",
    "    rprint(\"  → Fix identified issues\")\n",
    "    rprint(\"  → Re-run routing tests\")\n",
    "    rprint(\"  → Do not proceed to full workflow until routing passes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdc6c76",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Save Results\n",
    "\n",
    "Saving test results for documentation and future comparison.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef89979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Results to CSV\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(\"data/week_06\", exist_ok=True)\n",
    "\n",
    "# Save accuracy results\n",
    "accuracy_path = \"data/week_06/routing_accuracy.csv\"\n",
    "accuracy_results.to_csv(accuracy_path, index=False)\n",
    "rprint(f\"✓ Saved accuracy results: {accuracy_path}\")\n",
    "\n",
    "# Save consistency results\n",
    "consistency_path = \"data/week_06/routing_consistency.csv\"\n",
    "consistency_results.to_csv(consistency_path, index=False)\n",
    "rprint(f\"✓ Saved consistency results: {consistency_path}\")\n",
    "\n",
    "# Save summary\n",
    "summary_df = pd.DataFrame([{\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "    \"brand\": brand,\n",
    "    \"accuracy\": analysis['accuracy'],\n",
    "    \"consistency\": analysis['consistency'],\n",
    "    \"passes\": analysis['passes'],\n",
    "    \"failures\": len(analysis['failures']),\n",
    "    \"inconsistent_cases\": len(analysis['inconsistent_cases']),\n",
    "    \"total_test_cases\": len(routing_test_cases),\n",
    "    \"single_tool_cases\": len([tc for tc in routing_test_cases if len(tc['expected_tools']) == 1]),\n",
    "    \"multi_tool_cases\": len([tc for tc in routing_test_cases if len(tc['expected_tools']) > 1])\n",
    "}])\n",
    "\n",
    "summary_path = \"data/week_06/routing_summary.csv\"\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "rprint(f\"✓ Saved summary: {summary_path}\")\n",
    "\n",
    "rprint(f\"\\nAll results saved to data/week_06/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88373c20",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test Case Details\n",
    "\n",
    "Full details of all test cases for reference and debugging.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc5ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Full Results Table\n",
    "rprint(\"Full Accuracy Results:\")\n",
    "rprint(\"=\" * 60)\n",
    "display(accuracy_results[['query', 'expected_tools', 'actual_tools', 'correct', 'category']])\n",
    "\n",
    "rprint(\"\\nFull Consistency Results:\")\n",
    "rprint(\"=\" * 60)\n",
    "display(consistency_results[['query', 'expected_tools', 'actual_decisions', 'consistent', 'unique_decisions', 'mode_decision']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17a92ca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Multi-Tool Analysis\n",
    "\n",
    "Specific analysis of multi-tool routing cases (queries requiring multiple tools).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86379619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Tool Specific Analysis\n",
    "multi_tool_cases = accuracy_results[accuracy_results['category'] == 'Multi-tool']\n",
    "\n",
    "if len(multi_tool_cases) > 0:\n",
    "    rprint(\"=\" * 60)\n",
    "    rprint(\"MULTI-TOOL ROUTING ANALYSIS\")\n",
    "    rprint(\"=\" * 60)\n",
    "    \n",
    "    multi_accuracy = multi_tool_cases['correct'].mean()\n",
    "    rprint(f\"\\nMulti-tool accuracy: {multi_accuracy * 100:.1f}%\")\n",
    "    rprint(f\"Cases: {multi_tool_cases['correct'].sum()}/{len(multi_tool_cases)} correct\")\n",
    "    \n",
    "    rprint(\"\\nMulti-tool test cases:\")\n",
    "    for _, row in multi_tool_cases.iterrows():\n",
    "        status = \"✓\" if row['correct'] else \"✗\"\n",
    "        rprint(f\"\\n{status} {row['query'][:60]}...\")\n",
    "        rprint(f\"  Expected: {row['expected_tools']}\")\n",
    "        rprint(f\"  Actual: {row['actual_tools']}\")\n",
    "        if not row['correct']:\n",
    "            rprint(f\"  Issue: {row['reasoning']}\")\n",
    "else:\n",
    "    rprint(\"No multi-tool test cases defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1e883",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Notes and Observations\n",
    "\n",
    "**Key Findings:**\n",
    "- [Add observations after running tests]\n",
    "\n",
    "**Multi-Tool Coordination:**\n",
    "- [Note how well content planning agent handles queries needing multiple tools]\n",
    "\n",
    "**Potential Issues:**\n",
    "- [Note any patterns in failures, especially multi-tool cases]\n",
    "\n",
    "**Recommendations:**\n",
    "- [Suggestions for improvement if tests fail]\n",
    "\n",
    "---\n",
    "\n",
    "**Test completed:** [Date/Time auto-filled when run]\n",
    "\n",
    "**Next steps:** [Determined by pass/fail results]\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
