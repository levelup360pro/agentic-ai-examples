{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee636755",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\">\n",
    "  <a href=\"https://www.linkedin.com/company/100622063\" target=\"_blank\" title=\"Follow LevelUp360 on LinkedIn\">\n",
    "    <img src=\"../../assets/levelup360-inverted-logo-transparent.svg\" alt=\"LevelUp360\" width=\"220\">\n",
    "  </a>\n",
    "</p>\n",
    "\n",
    "# Marketing Team - Week 02: RAG Pipeline & Content Generation\n",
    "\n",
    "Production-ready RAG system for brand-aware content generation.\n",
    "\n",
    "---\n",
    "\n",
    "## What We're Testing\n",
    "\n",
    "**RAG Pipeline**: DocumentLoader → RAGHelper → VectorStore → PromptBuilder → LLM\n",
    "\n",
    "**Components**:\n",
    "- Document loading (brand guidelines, past posts)\n",
    "- Vector storage (ChromaDB with metadata filtering)\n",
    "- Prompt building (templates + RAG context + web search)\n",
    "- Content generation (LinkedIn/Facebook posts)\n",
    "- Evaluation (manual scoring against rubric)\n",
    "\n",
    "**Architecture**: 3-layer separation of concerns (load → process → store)\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "### Prerequisites\n",
    "- Python 3.10+\n",
    "- `.env` file with API keys (see `.env.example`)\n",
    "- Virtual environment in workspace root\n",
    "\n",
    "### Required Environment Variables\n",
    "```\n",
    "NOTE: The current LLMProvider class handles works with both Azure AI Foundry (AZURE_OPENAI env variables) and OpenRouter models. Update the class an needed if using a different provider.\n",
    "\n",
    "NOTE:OpenRouter doesn't offen embedding models, use Azure or your preferred provider for embedding models\n",
    "\n",
    "OPENROUTER_API_KEY=sk-...          # For OpenRouter\n",
    "AZURE_OPENAI_ENDPOINT=https://...  # For Azure\n",
    "AZURE_OPENAI_API_KEY=...\n",
    "TAVILY_API_KEY=...                 # For web search\n",
    "LANGSMITH_API_KEY=...              # Optional: tracing\n",
    "```\n",
    "\n",
    "### One-Time Setup (PowerShell)\n",
    "```powershell\n",
    "# From workspace root (this repo):\n",
    "python -m venv .venv\n",
    ".\\.venv\\Scripts\\Activate.ps1\n",
    "\n",
    "# If activation is blocked, run once as admin to allow scripts:\n",
    "Set-ExecutionPolicy -Scope CurrentUser RemoteSigned\n",
    "\n",
    "# Upgrade packaging tooling\n",
    "python -m pip install --upgrade pip setuptools wheel\n",
    "\n",
    "# Install project dependencies (unlocked). Prefer installing from requirements.txt\n",
    "# which should contain the top-level packages (no pinned versions).\n",
    "if (Test-Path ./requirements.txt) {\n",
    "  pip install -r requirements.txt\n",
    "} else {\n",
    "  # Fallback: install core packages used in the examples\n",
    "  pip install openai python-dotenv pydantic pandas pyyaml rich langsmith chromadb tavily-python tiktoken\n",
    "}\n",
    "\n",
    "```\n",
    "\n",
    "## Notebook Flow\n",
    "\n",
    "1. **Setup** - Import modules, initialize LLM/embedding clients\n",
    "2. **RAG Pipeline** - Load brand docs, chunk, generate embeddings, store in vector DB\n",
    "3. **Query Testing** - Test vector search with metadata filters\n",
    "4. **Prompt Building** - Build prompts with templates, RAG context, web search\n",
    "5. **Content Generation** - Generate LinkedIn posts with full context\n",
    "6. **Evaluation** - Score content against brand rubric\n",
    "\n",
    "**Data directories**:\n",
    "- `configs/` - Brand YAML file (test with the examples provided or replace with yours)\n",
    "- `data/chroma_db/` - Persistent vector store\n",
    "- `data/past_posts/` - Past Posts to upload to vector store\n",
    "- `data/week_02_scores.csv` - Evaluation scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053a80cf",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Import modules and initialize clients (LLM, embeddings, cost tracking)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25192663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "from rich import print as rprint\n",
    "from chromadb import Settings\n",
    "\n",
    "# Add marketing_team/src to path\n",
    "current_dir = Path.cwd()\n",
    "src_path = current_dir.parent / \"src\"\n",
    "if src_path.exists() and str(src_path) not in sys.path:\n",
    "    sys.path.insert(0, str(src_path))\n",
    "\n",
    "# Import all modules\n",
    "from utils.llm_client import LLMClient\n",
    "from utils.cost_tracker import CostTracker\n",
    "from utils.scoring import ScoringHelper\n",
    "from rag.vector_store import VectorStore\n",
    "from rag.rag_helper import RAGHelper\n",
    "from rag.document_loader import DocumentLoader\n",
    "from search.tavily_client import TavilySearchClient\n",
    "from prompts.templates import (\n",
    "    LINKEDIN_POST_ZERO_SHOT, \n",
    "    LINKEDIN_POST_FEW_SHOT,\n",
    "    LINKEDIN_ARTICLE,\n",
    "    FACEBOOK_POST_ZERO_SHOT,\n",
    "    FACEBOOK_POST_FEW_SHOT\n",
    ")\n",
    "from prompts.prompt_builder import PromptBuilder\n",
    "from generation.generator import ContentGenerator\n",
    "\n",
    "rprint(\"[green]✓ All modules imported[/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388736d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM clients\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress LangSmith warnings if tracing fails\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='langsmith')\n",
    "\n",
    "completion_client = LLMClient()\n",
    "completion_client.get_client(\"openrouter\")\n",
    "\n",
    "embedding_client = LLMClient()\n",
    "embedding_client.get_client(\"azure\")\n",
    "\n",
    "rprint(\"[green]✓ LLM clients initialized[/green]\")\n",
    "rprint(\"[dim]Note: LangSmith tracing errors can be safely ignored[/dim]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96ad9d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test: completion + cost tracking\n",
    "\n",
    "user_message = \"What are the most effective strategies for scaling enterprise cloud migrations while ensuring security, compliance, and cost control?\"\n",
    "messages = [{\"role\": \"user\", \"content\": user_message}]\n",
    "\n",
    "response = completion_client.get_completion(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "rprint(f\"\\nResponse: {response.content}\")\n",
    "rprint(f\"Cost: €{response.cost:.6f} | Latency: {response.latency:.2f}s\")\n",
    "\n",
    "tracker = CostTracker()\n",
    "summary = tracker.get_cost_summary()\n",
    "rprint(f\"\\nTotal cost today: €{summary.total_cost:.6f} ({summary.total_calls} calls)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5424646",
   "metadata": {},
   "source": [
    "## 2. RAG Pipeline\n",
    "\n",
    "Load brand docs → chunk → generate embeddings → store in vector DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867e6123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize RAG components\n",
    "loader = DocumentLoader(base_path=current_dir.parent)\n",
    "\n",
    "rag_helper = RAGHelper(\n",
    "    embedding_client=embedding_client,\n",
    "    embedding_model=\"text-embedding-3-small\",\n",
    "    chunk_size=150,\n",
    "    chunk_overlap=30,\n",
    "    chunk_threshold=150\n",
    ")\n",
    "\n",
    "settings = Settings(anonymized_telemetry=False)\n",
    "persist_dir = str(current_dir.parent / \"data\" / \"chroma_db\")\n",
    "vector_store = VectorStore(persist_directory=persist_dir, settings=settings)\n",
    "\n",
    "collection_name = \"marketing_content\"\n",
    "vector_store.clear_collection(collection_name)\n",
    "collection = vector_store.get_or_create_collection(\n",
    "    collection_name=collection_name,\n",
    "    metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "\n",
    "rprint(\"[green]✓ RAG components initialized[/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e169122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load brand past posts from subdirectories\n",
    "past_posts_dir = current_dir.parent / \"data\" / \"past_posts\"\n",
    "\n",
    "# Load itconsulting past posts \n",
    "itconsulting_docs = loader.load_files(\n",
    "    directory=past_posts_dir,\n",
    "    pattern=\"*itconsulting.md\",\n",
    "    metadata={\"brand\": \"itconsulting\", \"doc_type\": \"past_post\"},\n",
    "    recursive=True\n",
    ")\n",
    "\n",
    "# Load cosmetics past posts \n",
    "cosmetics_docs = loader.load_files(\n",
    "    directory=past_posts_dir,\n",
    "    pattern=\"*cosmetics.md\",\n",
    "    metadata={\"brand\": \"cosmetics\", \"doc_type\": \"past_post\"},\n",
    "    recursive=True\n",
    ")\n",
    "\n",
    "all_docs = itconsulting_docs + cosmetics_docs\n",
    "\n",
    "rprint(f\"Loaded {len(all_docs)} brand past posts ({len(itconsulting_docs)} IT Consulting, {len(cosmetics_docs)} Cosmetics)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324bb10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process and store documents (skip if already done)\n",
    "doc_count = vector_store.get_document_count(collection_name)\n",
    "\n",
    "if doc_count == 0:\n",
    "    rprint(\"Processing documents (chunking + embeddings)...\")\n",
    "    documents = rag_helper.prepare_past_posts(all_docs, verbose=True)\n",
    "    \n",
    "    rprint(f\"Adding {len(documents)} chunks to vector store...\")\n",
    "    count = vector_store.add_documents(collection_name, documents)\n",
    "    rprint(f\"[green]✓ Stored {count} chunks[/green]\")\n",
    "else:\n",
    "    rprint(f\"[yellow]Collection already has {doc_count} documents - skipping[/yellow]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a169acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test vector query (need to embed the query text first)\n",
    "query_text = \"What are the most effective strategies for scaling enterprise cloud migrations while ensuring security, compliance, and cost control?\"\n",
    "query_embedding = rag_helper.embed_batch([query_text])[0]\n",
    "\n",
    "query_results = vector_store.query(\n",
    "    collection_name=collection_name,\n",
    "    query_embeddings=query_embedding,\n",
    "    n_results=3,\n",
    "    where={\"brand\": \"itconsulting\"}\n",
    ")\n",
    "\n",
    "rprint(f\"\\n[cyan]Query:[/cyan] '{query_text}'\")\n",
    "rprint(f\"[green]Found {len(query_results.ids)} results[/green]\")\n",
    "for i, (metadata, text, distance) in enumerate(zip(query_results.metadatas, query_results.texts, query_results.distances), 1):\n",
    "    rprint(f\"{i}. Distance: {distance:.4f} | Metadata: {metadata} | Text: {text[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79940dcd",
   "metadata": {},
   "source": [
    "## 3. Prompt Building\n",
    "\n",
    "Build prompts with templates, optionally adding RAG context and web search results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eca93a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize prompt builder\n",
    "tavily_client = TavilySearchClient()\n",
    "prompt_builder = PromptBuilder(vector_store, rag_helper, tavily_client)\n",
    "\n",
    "# Load brand config\n",
    "brand_config_path = current_dir.parent / \"configs\" / \"itconsulting.yaml\"\n",
    "with open(brand_config_path, 'r', encoding='utf-8') as f:\n",
    "    brand_config = yaml.safe_load(f)\n",
    "\n",
    "topic = \"What are the most effective strategies for scaling enterprise cloud migrations while ensuring security, compliance, and cost control?\"\n",
    "brand = \"itconsulting\"\n",
    "\n",
    "rprint(f\"[green]✓ Prompt builder ready for brand: {brand_config['name']}[/green]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99c4435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Zero-shot (no RAG, no search)\n",
    "# This creates a basic prompt with only brand guidelines - no past examples or web data\n",
    "prompt_basic = prompt_builder.build_user_message(\n",
    "    collection_name=collection_name,\n",
    "    template=LINKEDIN_POST_ZERO_SHOT,\n",
    "    topic=topic,\n",
    "    brand=brand,\n",
    "    brand_config=brand_config,\n",
    "    include_rag=False,              # Don't retrieve past post examples\n",
    "    max_distance=0.50,              # Distance threshold for RAG similarity (not used here)\n",
    "    include_search=False,           # Don't add web search results\n",
    "    search_depth='basic',           # Search quality: 'basic' or 'advanced' (not used here)\n",
    "    search_type='general',          # Use 'general' search for no domain filters. Use 'technical', 'industry', 'news', 'documentation' to apply domain filters (see tavily_client.py)\n",
    "    llm_client=completion_client    # LLM used for query generation (not used here)\n",
    ")\n",
    "\n",
    "rprint(f\"\\n[cyan]Zero-shot prompt:[/cyan] {len(prompt_basic)} chars\")\n",
    "rprint(prompt_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf0a969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 2: With RAG context\n",
    "# This adds similar past posts from the vector database to guide style and content\n",
    "prompt_with_rag = prompt_builder.build_user_message(\n",
    "    collection_name=collection_name,\n",
    "    template=LINKEDIN_POST_ZERO_SHOT,\n",
    "    topic=topic,\n",
    "    brand=brand,\n",
    "    brand_config=brand_config,\n",
    "    include_rag=True,               # Retrieve similar past posts as examples\n",
    "    max_distance=0.50,              # Only use posts with similarity distance < 0.50 (closer = more similar)\n",
    "    include_search=False,\n",
    "    search_depth='basic',\n",
    "    search_type='general',\n",
    "    llm_client=completion_client\n",
    ")\n",
    "\n",
    "rprint(f\"\\n[cyan]Prompt with RAG:[/cyan] {len(prompt_with_rag)} chars\")\n",
    "rprint(prompt_with_rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969338fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 3: With RAG + web search\n",
    "# This adds both past posts AND current web search results for up-to-date information\n",
    "prompt_full = prompt_builder.build_user_message(\n",
    "    collection_name=collection_name,\n",
    "    template=LINKEDIN_POST_ZERO_SHOT,\n",
    "    topic=topic,\n",
    "    brand=brand,\n",
    "    brand_config=brand_config,\n",
    "    include_rag=True,               # Include past post examples\n",
    "    max_distance=0.50,              # Similarity threshold for retrieved posts\n",
    "    include_search=True,            # Add web search results for current data/trends\n",
    "    search_depth='advanced',        # Use 'advanced' for deeper web search (more sources, better quality)\n",
    "    search_type='general',          # Use 'general' search for no domain filters. Use 'technical', 'industry', 'news', 'documentation' to apply domain filters (see tavily_client.py)\n",
    "    llm_client=completion_client    # LLM generates optimized search queries\n",
    ")\n",
    "\n",
    "rprint(f\"\\n[cyan]Prompt with RAG + Search:[/cyan] {len(prompt_full)} chars\")\n",
    "rprint(prompt_full)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd55a84",
   "metadata": {},
   "source": [
    "## 4. Content Generation (Manual)\n",
    "\n",
    "Generate content using prompts directly and track costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcf2af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate LinkedIn post with full context\n",
    "post = [{\"role\": \"user\", \"content\": prompt_full}]\n",
    "\n",
    "response = completion_client.get_completion(\n",
    "    model=\"gpt-4o-mini\", \n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "rprint(\"\\n\" + \"=\"*70)\n",
    "rprint(\"[bold green]Generated LinkedIn Post:[/bold green]\")\n",
    "rprint(\"=\"*70)\n",
    "rprint(response.content)\n",
    "rprint(\"=\"*70)\n",
    "rprint(f\"\\nCost: €{response.cost:.6f} | Tokens: {response.input_tokens + response.output_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffdd162",
   "metadata": {},
   "source": [
    "## 4. Content Generation with ContentGenerator\n",
    "\n",
    "Use ContentGenerator to test different configurations and compare results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8878feb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize generator for LevelUp360\n",
    "generator = ContentGenerator(\n",
    "    llm_client=completion_client,\n",
    "    vector_store=vector_store,\n",
    "    rag_helper=rag_helper,\n",
    "    brand_config=brand_config,\n",
    "    collection_name=collection_name,\n",
    "    search_client=tavily_client\n",
    ")\n",
    "\n",
    "rprint(\"[green]✓ ContentGenerator initialized[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b107fe",
   "metadata": {},
   "source": [
    "### A/B Test: Compare RAG vs No RAG vs RAG+Search\n",
    "\n",
    "Generate the same post with 3 different configurations to compare quality and cost.\n",
    "\n",
    "**Key Parameters Explained:**\n",
    "- **`include_rag`**: Whether to retrieve similar past posts from vector DB as examples\n",
    "- **`rag_max_distance`**: Similarity threshold (0.0-1.0) - lower = more similar posts only\n",
    "- **`include_search`**: Whether to add current web search results\n",
    "- **`search_depth`**: Search quality - `'basic'` (fast) or `'advanced'` (better quality, more sources)\n",
    "- **`search_type`**: Search focus - use `'general'` for no domain filters, or specify domain filters: `'technical'`, `'industry'`, `'news'`, or `'documentation'` (see tavily_client.py).\n",
    "- **`system_message`**: Custom instructions to guide the LLM's style and behavior\n",
    "- **`temperature`**: Creativity level (0.0 = deterministic, 1.0 = very creative)\n",
    "- **`model`**: LLM to use - `gpt-4o-mini` (fast/cheap) or `gpt-4o` (best quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802e879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test topic\n",
    "test_topic = \"What are the most effective strategies for scaling enterprise cloud migrations while ensuring security, compliance, and cost control?\"\n",
    "\n",
    "# System message: Instructions that guide the LLM's writing style and behavior\n",
    "# Customize this to enforce your brand voice and content requirements\n",
    "system_message = \"\"\"You are a professional content generator.\n",
    "\n",
    "Write clear, professional LinkedIn posts that:\n",
    "- Start with concrete data or specific examples\n",
    "- Use precise technical language\n",
    "- Avoid buzzwords and vague phrases\n",
    "\"\"\"\n",
    "\n",
    "# Generate WITHOUT RAG\n",
    "# This uses only the brand guidelines - no past examples or web data\n",
    "rprint(\"\\n[cyan]Generating WITHOUT RAG...[/cyan]\")\n",
    "result_no_rag = generator.generate(\n",
    "    topic=test_topic,\n",
    "    content_type=\"linkedin_post\",     # Template type: linkedin_post, linkedin_article, facebook_post\n",
    "    include_rag=False,                # Don't retrieve past posts\n",
    "    include_search=False,             # Don't add web search\n",
    "    search_depth='basic',             # Search quality (not used here)\n",
    "    search_type='general',            # Search type (not used here)\n",
    "    model=\"gpt-4o-mini\",              # LLM model (gpt-4o-mini is faster/cheaper, gpt-4o is better quality)\n",
    "    system_message=system_message,    # Custom instructions for content style\n",
    "    temperature=0.7                   # Creativity level (0.0=deterministic, 1.0=creative)\n",
    ")\n",
    "\n",
    "rprint(\"\\n\" + \"=\"*70)\n",
    "rprint(\"[bold yellow]WITHOUT RAG:[/bold yellow]\")\n",
    "rprint(\"=\"*70)\n",
    "rprint(result_no_rag['content'])\n",
    "rprint(\"=\"*70)\n",
    "rprint(f\"Cost: €{result_no_rag['metadata']['cost']:.6f} | Latency: {result_no_rag['metadata']['latency']:.2f}s\")\n",
    "rprint(f\"Tokens: {result_no_rag['metadata']['input_tokens']} in / {result_no_rag['metadata']['output_tokens']} out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bd14d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate WITH RAG\n",
    "# This retrieves similar past posts from the vector DB to guide style and content\n",
    "rprint(\"\\n[cyan]Generating WITH RAG...[/cyan]\")\n",
    "result_with_rag = generator.generate(\n",
    "    topic=test_topic,\n",
    "    content_type=\"linkedin_post\",\n",
    "    include_rag=True,                 # Retrieve similar past posts as context\n",
    "    rag_max_distance=0.50,            # Only use posts with similarity < 0.50 (adjust based on your corpus)\n",
    "    include_search=False,\n",
    "    search_depth='basic',\n",
    "    search_type='general',\n",
    "    model=\"gpt-4o-mini\",\n",
    "    system_message=system_message,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "rprint(\"\\n\" + \"=\"*70)\n",
    "rprint(\"[bold green]WITH RAG:[/bold green]\")\n",
    "rprint(\"=\"*70)\n",
    "rprint(result_with_rag['content'])\n",
    "rprint(\"=\"*70)\n",
    "rprint(f\"Cost: €{result_with_rag['metadata']['cost']:.6f} | Latency: {result_with_rag['metadata']['latency']:.2f}s\")\n",
    "rprint(f\"Tokens: {result_with_rag['metadata']['input_tokens']} in / {result_with_rag['metadata']['output_tokens']} out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ed7156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate WITH RAG + SEARCH\n",
    "# This combines past post examples with current web search for up-to-date, brand-consistent content\n",
    "rprint(\"\\n[cyan]Generating WITH RAG + SEARCH...[/cyan]\")\n",
    "result_with_search = generator.generate(\n",
    "    topic=test_topic,\n",
    "    content_type=\"linkedin_post\",\n",
    "    include_rag=True,                 # Include past posts for brand voice\n",
    "    rag_max_distance=0.50,            # Similarity threshold for RAG\n",
    "    include_search=True,              # Add web search for current data\n",
    "    search_depth='advanced',          # Use advanced search for better quality results (more sources, deeper analysis)\n",
    "    search_type='general',            # Use 'general' search for no domain filters. Use 'technical', 'industry', 'news', 'documentation' to apply domain filters (see tavily_client.py)\n",
    "    model=\"gpt-4o\",                   # Use gpt-4o for best quality (more expensive than gpt-4o-mini)\n",
    "    system_message=system_message,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "rprint(\"\\n\" + \"=\"*70)\n",
    "rprint(\"[bold magenta]WITH RAG + SEARCH:[/bold magenta]\")\n",
    "rprint(\"=\"*70)\n",
    "rprint(result_with_search['content'])\n",
    "rprint(\"=\"*70)\n",
    "rprint(f\"Cost: €{result_with_search['metadata']['cost']:.6f} | Latency: {result_with_search['metadata']['latency']:.2f}s\")\n",
    "rprint(f\"Tokens: {result_with_search['metadata']['input_tokens']} in / {result_with_search['metadata']['output_tokens']} out\")\n",
    "\n",
    "rprint(\"\\n[green]✓ A/B test complete - compare the 3 variants above[/green]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04628950",
   "metadata": {},
   "source": [
    "### Manual Inspection\n",
    "\n",
    "Compare the 3 outputs above and note your observations:\n",
    "\n",
    "**1. WITHOUT RAG:**\n",
    "- Does it capture brand voice? (Y/N)\n",
    "- Are facts accurate? (Y/N)\n",
    "- Does it feel generic? (Y/N)\n",
    "\n",
    "**2. WITH RAG:**\n",
    "- Better brand voice than without RAG? (Y/N)\n",
    "- More specific/less generic? (Y/N)\n",
    "- Worth the extra cost/latency? (Y/N)\n",
    "\n",
    "**3. WITH RAG + SEARCH:**\n",
    "- Does search add value? (Y/N)\n",
    "- Are web sources relevant? (Y/N)\n",
    "- Worth the extra cost/latency? (Y/N)\n",
    "\n",
    "**Initial observations:** [Write your notes here after running above cells]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355ae30e",
   "metadata": {},
   "source": [
    "## 5. Evaluation & Scoring\n",
    "\n",
    "Use the cells above to generate posts for different topics (one at a time), and evaluate them against the criteria below. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
