{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "intro-title",
      "metadata": {},
      "source": [
        "<p style=\"text-align:center\">\n",
        "  <a href=\"https://www.linkedin.com/company/100622063\" target=\"_blank\" title=\"Follow LevelUp360 on LinkedIn\">\n",
        "    <img src=\"../../assets/levelup360-inverted-logo-transparent.svg\" alt=\"LevelUp360\" width=\"220\">\n",
        "  </a>\n",
        "</p>\n",
        "\n",
        "# Marketing Team – Week 04: Agentic Content Generation Workflow (LangGraph)\n",
        "\n",
        "This notebook validates the full agentic workflow implemented in Week 4:\n",
        "\n",
        "**content_planning → tools → content_generation → content_evaluation → (loop or end)**\n",
        "\n",
        "We use a content planning node to decide tools (RAG, Web, both, none), LangGraph’s ToolNode to execute tools, a generation node that synthesizes tool contexts into a draft, and an evaluation node that scores quality and controls a regeneration loop (iteration_count vs max_iterations, threshold). For deterministic refinements, the generation node applies an optimization system message on regeneration.\n",
        "\n",
        "---\n",
        "\n",
        "## What We’re Testing\n",
        "\n",
        "- Tool routing: content planning node chooses `rag_search`, `web_search`, both, or none\n",
        "- Agentic execution: ToolNode fetches contexts; generation consumes ToolMessages (no duplicate calls)\n",
        "- Draft creation: ContentGenerator.generate_from_context builds prompts via PromptBuilder\n",
        "- Evaluation & loop: ContentEvaluator scores content; evaluation node increments `iteration_count`, sets `meets_quality_threshold`, and routes regenerate/end\n",
        "- Optional deterministic refinement: `evaluator_optimizer` applies `models.content_optimization.system_message` during regeneration\n",
        "\n",
        "**Architecture:** content_planning (LLM tool-calling) → ToolNode (exec) → content_generation (uses tool_contexts) → content_evaluation (Critique + loop control)\n",
        "\n",
        "---\n",
        "\n",
        "## Environment Setup\n",
        "\n",
        "### Prerequisites\n",
        "- Python 3.10+\n",
        "- `.env` with API keys (see below)\n",
        "- Virtual environment in repo root\n",
        "- VS Code Jupyter extension (or JupyterLab)\n",
        "\n",
        "### Required Environment Variables\n",
        "    # OpenRouter\n",
        "    OPENROUTER_API_KEY=sk-...\n",
        "\n",
        "    # Azure (optional for embeddings)\n",
        "    AZURE_OPENAI_ENDPOINT=https://...\n",
        "    AZURE_OPENAI_API_KEY=...\n",
        "    AZURE_OPENAI_API_VERSION=...\n",
        "\n",
        "    # Web search\n",
        "    TAVILY_API_KEY=...\n",
        "\n",
        "    # LangSmith (optional tracing)\n",
        "    LANGSMITH_TRACING_V2=true\n",
        "    LANGSMITH_API_KEY=...\n",
        "    LANGSMITH_PROJECT=levelup360-marketing-team\n",
        "    LANGSMITH_ENDPOINT=https://api.smith.langchain.com\n",
        "\n",
        "    # (Optional) Pricing for local cost tracking\n",
        "    GPT4O_INPUT_PRICE_PER_1K=...\n",
        "    GPT4O_OUTPUT_PRICE_PER_1K=...\n",
        "    EMBEDDING_PRICE_PER_1K=...\n",
        "    TAVILY_PRICE_PER_CALL=...\n",
        "\n",
        "### One-Time Setup (PowerShell on Windows)\n",
        "    # From workspace root:\n",
        "    python -m venv .venv\n",
        "    .\\\\.venv\\\\Scripts\\\\Activate.ps1\n",
        "\n",
        "    # If activation blocked, run once (PowerShell):\n",
        "    Set-ExecutionPolicy -Scope CurrentUser RemoteSigned\n",
        "\n",
        "    # Upgrade tooling\n",
        "    python -m pip install --upgrade pip setuptools wheel\n",
        "\n",
        "    # Install dependencies\n",
        "    if (Test-Path ./requirements.txt) {\n",
        "      pip install -r requirements.txt\n",
        "    } else {\n",
        "      pip install openai python-dotenv pydantic langsmith pandas chromadb tavily-python tiktoken pyyaml rich ipykernel langchain_openai langgraph langchain-core langchain\n",
        "    }\n",
        "\n",
        "    # Editable install (so `from src...` works everywhere)\n",
        "    # Ensure pyproject.toml exists at the project root where src/ lives:\n",
        "    #   [build-system]\n",
        "    #   requires = [\"setuptools>=68\", \"wheel\"]\n",
        "    #   build-backend = \"setuptools.build_meta\"\n",
        "    #\n",
        "    #   [project]\n",
        "    #   name = \"marketing-team\"\n",
        "    #   version = \"0.1.0\"\n",
        "    #   requires-python = \">=3.10\"\n",
        "    #\n",
        "    #   [tool.setuptools.packages.find]\n",
        "    #   where = [\".\"]\n",
        "    #   include = [\"src*\"]\n",
        "    pip install -e .\n",
        "\n",
        "    # (Optional) Register kernel\n",
        "    python -m ipykernel install --user --name marketing-team --display-name \"Python (marketing-team)\"\n",
        "\n",
        "### One-Time Setup (macOS/Linux bash)\n",
        "    # From workspace root:\n",
        "    python -m venv .venv\n",
        "    source .venv/bin/activate\n",
        "\n",
        "    # Upgrade tooling\n",
        "    python -m pip install --upgrade pip setuptools wheel\n",
        "\n",
        "    # Install dependencies\n",
        "    if [ -f ./requirements.txt ]; then\n",
        "      pip install -r requirements.txt\n",
        "    else\n",
        "      pip install openai python-dotenv pydantic langsmith pandas chromadb tavily-python tiktoken pyyaml rich ipykernel langgraph langchain-core\n",
        "    fi\n",
        "\n",
        "    # Editable install (-e) so `from src...` imports work in scripts/notebooks/tests\n",
        "    pip install -e .\n",
        "\n",
        "    # (Optional) Register kernel\n",
        "    python -m ipykernel install --user --name marketing-team --display-name \"Python (marketing-team)\"\n",
        "\n",
        "### Verify Installation\n",
        "    # Should print \"OK\" without ImportError\n",
        "    python -c \"import src; from src.agents.graph import build_content_workflow; print('OK')\"\n",
        "\n",
        "---\n",
        "\n",
        "## Notebook Flow\n",
        "\n",
        "1. **Setup** – Autoreload, env, logging  \n",
        "2. **Build Workflow** – `build_content_workflow(brand)` compiles the LangGraph app  \n",
        "3. **Helpers** – Utilities to run scenarios and render message flow, draft, evaluation  \n",
        "4. **Scenarios** – RAG-only, Web-only, Both, None  \n",
        "5. **Optional: Optimization Loop** – Switch to `evaluator_optimizer` to validate deterministic refinement behavior  \n",
        "6. **Summary** – Inspect loop stats, metadata, outcomes; capture for documentation\n",
        "\n",
        "---\n",
        "\n",
        "## Data & Config\n",
        "\n",
        "- `config/brands/` – Brand YAMLs (models, retrieval, formatting, voice, CTA)  \n",
        "- `data/chroma_db/` – Vector store persistence (RAG)  \n",
        "- `outputs/week4_validation/` – Optional export path for results\n",
        "\n",
        "---\n",
        "\n",
        "## Key Settings Per Run\n",
        "\n",
        "- `template`: e.g., `LINKEDIN_POST_ZERO_SHOT`, `LINKEDIN_LONG_POST_ZERO_SHOT`, `BLOG_POST`  \n",
        "- `pattern`: `single_pass`, `reflection`, `evaluator_optimizer`  \n",
        "- `max_iterations`: int (e.g., 3)  \n",
        "- `quality_threshold`: float (e.g., 7.0)  \n",
        "- `use_cot`: bool; adds reasoning scaffold to the prompt (not included in output)  \n",
        "- `brand`: `\"levelup360\"` or `\"ossie_naturals\"`\n",
        "\n",
        "---\n",
        "\n",
        "## Expected Outputs\n",
        "\n",
        "- Message flow with clear tool routing (`AIMessage.tool_calls`, `ToolMessage`s)  \n",
        "- Draft content (preview)  \n",
        "- Evaluation summary with score, reasoning, violations (if any)  \n",
        "- Loop stats: `iteration_count`, `max_iterations`, `meets_quality_threshold`  \n",
        "- Generation/Evaluation metadata (model, cost, latency, tokens) if available\n",
        "\n",
        "---\n",
        "\n",
        "## Notes\n",
        "\n",
        "- Agentic path: tools are called once by content planning; generation consumes tool results; no duplicate tool calls.  \n",
        "- Deterministic refinement: when `evaluator_optimizer` is used and the draft fails threshold, generation uses `models.content_optimization.system_message` during regeneration.  \n",
        "- Routing evaluator (from Week 4) remains separate; this notebook focuses on end-to-end content generation and evaluation loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "setup-env",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-11-16 13:29:23,653 | INFO | week4_notebook | Environment & logging initialized\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import logging\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s | %(levelname)s | %(name)s | %(message)s')\n",
        "logger = logging.getLogger(\"week4_notebook\")\n",
        "logger.info(\"Environment & logging initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "imports",
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Dict, Any\n",
        "from pprint import pprint\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage, ToolMessage\n",
        "\n",
        "from src.utils.config_loader import load_brand_config\n",
        "from src.agents.graphs.content_generation_graph import build_content_workflow\n",
        "from src.agents.states.content_generation_state import ContentGenerationState\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "build-app",
      "metadata": {},
      "source": [
        "## Build Workflow App\n",
        "\n",
        "We build the agentic content workflow for a given brand.\n",
        "The compiled app executes the full graph with a single `invoke()` call.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "build-workflow",
      "metadata": {},
      "outputs": [],
      "source": [
        "brand = \"itconsulting\"  \n",
        "brand_config = load_brand_config(brand)\n",
        "\n",
        "app = build_content_workflow(brand)\n",
        "logger.info(\"Workflow built and compiled\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "helpers-title",
      "metadata": {},
      "source": [
        "## Helpers\n",
        "\n",
        "Utility functions to run a scenario and display the message flow, the generated draft, and the evaluation outcome.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "helpers",
      "metadata": {},
      "outputs": [],
      "source": [
        "def print_messages(messages):\n",
        "    print(\"\\n--- Message Flow ---\")\n",
        "    for i, m in enumerate(messages):\n",
        "        mtype = type(m).__name__\n",
        "        line = f\"{i:02d}: {mtype}\"\n",
        "        if isinstance(m, AIMessage):\n",
        "            tool_calls = getattr(m, 'tool_calls', None)\n",
        "            if tool_calls:\n",
        "                tool_names = [tc.get('name') for tc in tool_calls]\n",
        "                line += f\" (calls: {tool_names})\"\n",
        "        if isinstance(m, ToolMessage):\n",
        "            line += f\" (tool: {getattr(m, 'name', 'unknown')})\"\n",
        "        print(line)\n",
        "\n",
        "def run_scenario(topic: str, *, thread_id: str, template: str = \"LINKEDIN_POST_ZERO_SHOT\", use_cot: bool = False,\n",
        "                 max_iterations: int = 3, quality_threshold: float = 7.0, pattern: str = \"single_pass\") -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Run a full end-to-end scenario:\n",
        "      content_planning → tools → content_generation → content_evaluation → (loop or end)\n",
        "    Returns the final state.\n",
        "    \"\"\"\n",
        "    initial_state: ContentGenerationState = ContentGenerationState(\n",
        "        messages=[HumanMessage(content=topic)],\n",
        "        topic=topic,\n",
        "        brand=brand,\n",
        "        brand_config=brand_config,\n",
        "        template=template,\n",
        "        use_cot=use_cot,\n",
        "        draft_content=\"\",\n",
        "        critique=None,\n",
        "        iteration_count=0,\n",
        "        max_iterations=max_iterations,\n",
        "        quality_threshold=quality_threshold,\n",
        "        meets_quality_threshold=None,\n",
        "        generation_metadata=None,\n",
        "        evaluation_metadata=None,\n",
        "        pattern=pattern,\n",
        "    )\n",
        "\n",
        "    result_state = app.invoke(initial_state, config={\"configurable\": {\"thread_id\": thread_id}})\n",
        "    return result_state\n",
        "\n",
        "def show_result(state: Dict[str, Any]):\n",
        "    print_messages(state.get(\"messages\", []))\n",
        "    draft = state.get(\"draft_content\", \"\")\n",
        "    print(\"\\n--- Draft (first 600 chars) ---\\n\")\n",
        "    print(draft[:600])\n",
        "    if len(draft) > 600:\n",
        "        print(f\"... ({len(draft) - 600} more chars)\")\n",
        "\n",
        "    print(\"\\n--- Evaluation ---\\n\")\n",
        "    critique = state.get(\"critique\")\n",
        "    if critique:\n",
        "        score = getattr(critique, \"average_score\", None)\n",
        "        meets = state.get(\"meets_quality_threshold\")\n",
        "        print(f\"Score (adjusted to dimension's weight): {('%.2f' % score) if isinstance(score, (int, float)) else 'N/A'} | Meets threshold: {meets}\")\n",
        "\n",
        "        try:\n",
        "            scores = critique.scores\n",
        "            print(\"Dimension scores:\", {k: f\"{v:.2f}\" for k, v in scores.items()})\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "        reasoning = getattr(critique, \"reasoning\", None)\n",
        "        if reasoning:\n",
        "            print(\"Reasoning:\")\n",
        "            print(reasoning)\n",
        "        violations = getattr(critique, \"violations\", None) or []\n",
        "        if violations:\n",
        "            print(\"Violations:\")\n",
        "            for v in violations:\n",
        "                print(f\"- {v}\")\n",
        "    else:\n",
        "        print(\"No critique present (possibly ended before evaluation).\")\n",
        "\n",
        "    print(\"\\n--- Loop Stats ---\\n\")\n",
        "    print(\n",
        "        f\"iteration_count={state.get('iteration_count')} | \"\n",
        "        f\"max_iterations={state.get('max_iterations')} | \"\n",
        "        f\"quality_threshold={state.get('quality_threshold')}\"\n",
        "    )\n",
        "\n",
        "    gen_meta = state.get(\"generation_metadata\")\n",
        "    eval_meta = state.get(\"evaluation_metadata\")\n",
        "    if gen_meta:\n",
        "        from pprint import pprint\n",
        "        print(\"\\nGeneration metadata:\")\n",
        "        sel = {k: gen_meta.get(k) for k in [\"model\", \"cost\", \"latency\", \"input_tokens\", \"output_tokens\", \"pattern\", \"iterations\"] if gen_meta.get(k) is not None}\n",
        "        pprint(sel)\n",
        "    if eval_meta:\n",
        "        from pprint import pprint\n",
        "        print(\"\\nEvaluation metadata:\")\n",
        "        pprint(eval_meta)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "test1-title",
      "metadata": {},
      "source": [
        "## Test 1 — RAG-only scenario (Internal brand content)\n",
        "\n",
        "Expected: content_planning calls `rag_search`; tools execute; generation uses RAG context; evaluation runs; loop ends when threshold met or max iterations reached.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test1",
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_rag = \"Create a post about our AI governance approach at LevelUp360\"\n",
        "state_rag = run_scenario(\n",
        "    topic_rag,\n",
        "    thread_id=\"w4_rag_1\",\n",
        "    template=\"LINKEDIN_POST_ZERO_SHOT\",\n",
        "    use_cot=True,\n",
        "    max_iterations=3,\n",
        "    quality_threshold=7.0,\n",
        ")\n",
        "show_result(state_rag)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "test2-title",
      "metadata": {},
      "source": [
        "## Test 2 — Web-only scenario (Current events)\n",
        "\n",
        "Expected: content_planning calls `web_search`; generation uses web context.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test2",
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_web = \"Create a post about the latest AI regulation news in 2025\"\n",
        "state_web = run_scenario(\n",
        "    topic_web,\n",
        "    thread_id=\"w4_web_1\",\n",
        "    template=\"LINKEDIN_POST_ZERO_SHOT\",\n",
        "    use_cot=False,\n",
        "    max_iterations=3,\n",
        "    quality_threshold=7.0,\n",
        ")\n",
        "show_result(state_web)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "test3-title",
      "metadata": {},
      "source": [
        "## Test 3 — Both tools scenario (Internal + Industry)\n",
        "\n",
        "Expected: content_planning calls both `rag_search` and `web_search`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test3",
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_both = \"Compare our AI governance approach to current industry standards in 2025\"\n",
        "state_both = run_scenario(\n",
        "    topic_both,\n",
        "    thread_id=\"w4_both_1\",\n",
        "    template=\"LINKEDIN_POST_ZERO_SHOT\",\n",
        "    use_cot=True,\n",
        "    max_iterations=3,\n",
        "    quality_threshold=7.0,\n",
        ")\n",
        "show_result(state_both)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "test4-title",
      "metadata": {},
      "source": [
        "## Test 4 — No-tools scenario (Opinion)\n",
        "\n",
        "Expected: content_planning calls no tools; generation runs direct.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "test4",
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_none = \"Share my opinion on why AI governance matters for small teams\"\n",
        "state_none = run_scenario(\n",
        "    topic_none,\n",
        "    thread_id=\"w4_none_1\",\n",
        "    template=\"LINKEDIN_POST_ZERO_SHOT\",\n",
        "    use_cot=False,\n",
        "    max_iterations=3,\n",
        "    quality_threshold=7.0,\n",
        ")\n",
        "show_result(state_none)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "summary",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "- content_planning routed correctly (RAG / Web / Both / None)\n",
        "- tool results flowed into content_generation via ToolMessages → tool_contexts\n",
        "- content_evaluation returned Critique, updated iteration_count, and set meets_quality_threshold\n",
        "- Conditional routing looped to regeneration when threshold unmet and iterations left\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv (3.12.1)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
