{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c87df846",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "<p style=\"text-align:center\">\n",
    "  <a href=\"https://www.linkedin.com/company/100622063\" target=\"_blank\" title=\"Follow LevelUp360 on LinkedIn\">\n",
    "    <img src=\"../../assets/levelup360-inverted-logo-transparent.svg\" alt=\"LevelUp360\" width=\"220\">\n",
    "  </a>\n",
    "</p>\n",
    "\n",
    "# Marketing Team – Week 06: Agentic Content Generation Workflow (Microsoft Agent Framework)\n",
    "\n",
    "This notebook validates the full agentic workflow implemented in Week 6 using the **Microsoft Agent Framework**:\n",
    "\n",
    "**content_planning → tools → content_generation → content_evaluation → (loop or end)**\n",
    "\n",
    "We use a content planning executor to decide tools (RAG, Web, both, none), dedicated tool executors to fetch contexts, a generation executor that synthesizes the user prompt and context retrireved by the tool executor (if any) into a draft, and an evaluation executor that scores quality and controls a regeneration loop (iteration_count vs max_iterations, threshold).\n",
    "\n",
    "---\n",
    "\n",
    "## What We’re Testing\n",
    "\n",
    "- Tool routing: content planning executor chooses `rag_search`, `web_search`, both, or none\n",
    "- Agentic execution: tool executors fetch contexts; generation consumes tool outputs (no duplicate calls)\n",
    "- Draft creation: generation executor builds prompts via brand/template config\n",
    "- Evaluation & loop: evaluation executor scores content; loop control fields on the shared state\n",
    "\n",
    "**Architecture:** Microsoft Agent Framework Workflow → Executors:\n",
    "\n",
    "- `StartExecutor` → seeds `ContentThreadState` from a user topic\n",
    "- `ContentPlanningExecutor` → decides tools and records `planning_decision`\n",
    "- Tool path (RAG / Web / both / none) → fills `tool_contexts` / research fields\n",
    "- `ContentGenerationExecutor` → generates draft content into `state.content`\n",
    "- `ContentEvaluationExecutor` → critiques draft, updates loop control fields\n",
    "- `FinalStateExecutor` → yields final `ContentThreadState` as workflow output\n",
    "\n",
    "---\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "### Prerequisites\n",
    "- Python 3.10+\n",
    "- `.env` with API keys (see below)\n",
    "- Virtual environment in repo root\n",
    "- VS Code Jupyter extension (or JupyterLab)\n",
    "\n",
    "### Required Environment Variables\n",
    "    # OpenRouter / Azure / other model provider\n",
    "    OPENROUTER_API_KEY=sk-...              # or provider-specific key\n",
    "\n",
    "    # Azure (optional for embeddings / models)\n",
    "    AZURE_OPENAI_ENDPOINT=https://...\n",
    "    AZURE_OPENAI_API_KEY=...\n",
    "    AZURE_OPENAI_API_VERSION=...\n",
    "\n",
    "    # Web search\n",
    "    TAVILY_API_KEY=...\n",
    "\n",
    "    # LangSmith (optional tracing)\n",
    "    LANGSMITH_TRACING_V2=true\n",
    "    LANGSMITH_API_KEY=...\n",
    "    LANGSMITH_PROJECT=...\n",
    "    LANGSMITH_ENDPOINT=https://api.smith.langchain.com\n",
    "\n",
    "    # (Optional) Pricing for local cost tracking\n",
    "    GPT4O_INPUT_PRICE_PER_1K=...\n",
    "    GPT4O_OUTPUT_PRICE_PER_1K=...\n",
    "    EMBEDDING_PRICE_PER_1K=...\n",
    "    TAVILY_PRICE_PER_CALL=...\n",
    "\n",
    "### One-Time Setup (PowerShell on Windows)\n",
    "    # From workspace root:\n",
    "    python -m venv .venv\n",
    "    .\\\\.venv\\\\Scripts\\\\Activate.ps1\n",
    "\n",
    "    # If activation blocked, run once (PowerShell):\n",
    "    Set-ExecutionPolicy -Scope CurrentUser RemoteSigned\n",
    "\n",
    "    # Upgrade tooling\n",
    "    python -m pip install --upgrade pip setuptools wheel\n",
    "\n",
    "    # Install dependencies (root project + marketing example)\n",
    "    pip install -e .\n",
    "    pip install -r examples/marketing_team/requirements.txt\n",
    "\n",
    "    # (Optional) Register kernel\n",
    "    python -m ipykernel install --user --name marketing-team --display-name \"Python (marketing-team)\"\n",
    "\n",
    "### One-Time Setup (macOS/Linux bash)\n",
    "    # From workspace root:\n",
    "    python -m venv .venv\n",
    "    source .venv/bin/activate\n",
    "\n",
    "    # Upgrade tooling\n",
    "    python -m pip install --upgrade pip setuptools wheel\n",
    "\n",
    "    # Install dependencies\n",
    "    pip install -e .\n",
    "    pip install -r examples/marketing_team/requirements.txt\n",
    "\n",
    "    # (Optional) Register kernel\n",
    "    python -m ipykernel install --user --name marketing-team --display-name \"Python (marketing-team)\"\n",
    "\n",
    "### Verify Installation\n",
    "    # Should import the Agent Framework workflow without error\n",
    "    python -c \"from src.orchestration.microsoft_agent_framework.workflows.content_generation_workflow import build_content_generation_workflow; print('OK')\"\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Flow\n",
    "\n",
    "1. **Setup** – Autoreload, env, logging  \n",
    "2. **Build Workflow** – `build_content_generation_workflow(brand)` compiles the Microsoft Agent Framework workflow  \n",
    "3. **Helpers** – Utilities to run scenarios and render messages, draft, evaluation  \n",
    "4. **Scenarios** – RAG-only, Web-only, Both, None  \n",
    "5. **Summary** – Inspect loop stats, metadata, outcomes; capture for documentation\n",
    "\n",
    "---\n",
    "\n",
    "## Data & Config\n",
    "\n",
    "- `examples/marketing_team/configs/` – Brand YAMLs (models, retrieval, formatting, voice, CTA)  \n",
    "- `examples/marketing_team/data/chroma_db/` – Vector store persistence (RAG)  \n",
    "- `examples/marketing_team/data/week_06/` – Optional export path for results \n",
    "\n",
    "---\n",
    "\n",
    "## Key Settings Per Run\n",
    "\n",
    "- `template`: e.g., `LINKEDIN_POST_ZERO_SHOT`, `LINKEDIN_LONG_POST_ZERO_SHOT`, `BLOG_POST`  \n",
    "- `max_iterations`: int (e.g., 3)  \n",
    "- `quality_threshold`: float (e.g., 7.0)  \n",
    "- `use_cot`: bool; adds reasoning scaffold to the prompt (not included in output)  \n",
    "- `brand`: i.e. `\"itconsulting\"` or `\"aurora\"` (from the provided examples)\n",
    "\n",
    "---\n",
    "\n",
    "## Expected Outputs\n",
    "\n",
    "- Message flow captured in `ContentThreadState.messages` with tool routing decisions  \n",
    "- Draft content (preview) stored in `ContentThreadState.content`  \n",
    "- Evaluation summary with score, reasoning, violations (if any)  \n",
    "- Loop stats: `iteration_count`, `max_iterations`, `meets_quality_threshold`  \n",
    "- Generation/Evaluation metadata (model, cost, latency, tokens) where available\n",
    "\n",
    "---\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Routing evaluator (Week 6 routing tests) remains separate; this notebook focuses on **end-to-end content generation and evaluation loop** using the new workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e4c486",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s | %(levelname)s | %(name)s | %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(\"week6_agentic_content_generation\")\n",
    "logger.info(\"Environment & logging initialized (Week 6 – Agent Framework)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa5f41e",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, Any, List\n",
    "from rich import print as rprint\n",
    "\n",
    "from src.core.utils.config_loader import load_brand_config\n",
    "from src.orchestration.microsoft_agent_framework.workflows.content_generation_workflow import (\n",
    "    build_content_generation_workflow,\n",
    ")\n",
    "from src.orchestration.microsoft_agent_framework.thread_states.content_thread_state import (\n",
    "    ContentThreadState,\n",
    ")\n",
    "\n",
    "logger.info(\"Imports loaded (brand config, workflow builder, thread state)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64acb946",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Build Workflow (Microsoft Agent Framework)\n",
    "\n",
    "We build the agentic content workflow for a given brand using the Microsoft Agent Framework.\n",
    "The compiled workflow executes the full executor graph with a single `run()` call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690f9d08",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Choose brand for content generation\n",
    "brand = \"itconsulting\"  \n",
    "brand_config = load_brand_config(brand)\n",
    "\n",
    "# Build the Microsoft Agent Framework content generation workflow\n",
    "workflow = build_content_generation_workflow(brand=brand)\n",
    "logger.info(\"Microsoft Agent Framework content generation workflow built and compiled\")\n",
    "\n",
    "# Debug: inspect workflow structure\n",
    "logger.info(f\"Workflow executors: {list(workflow.executors.keys())}\")\n",
    "logger.info(f\"Workflow edge_groups: {len(workflow.edge_groups)}\")\n",
    "for idx, eg in enumerate(workflow.edge_groups):\n",
    "    logger.info(f\"  EdgeGroup[{idx}]: {type(eg).__name__}, edges={len(eg.edges) if hasattr(eg, 'edges') else 'N/A'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1062747b",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Helpers\n",
    "\n",
    "Utilities to run a scenario and display the message flow, the generated draft, and the evaluation outcome from the `ContentThreadState` returned by the workflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6a6588",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def _safe_get_thread_state_from_result(result: Any) -> ContentThreadState:\n",
    "    # Direct: result.state\n",
    "    if hasattr(result, \"state\") and isinstance(result.state, ContentThreadState):\n",
    "        return result.state\n",
    "\n",
    "    # Primary: result.get_outputs()[0].state\n",
    "    if hasattr(result, \"get_outputs\"):\n",
    "        outs = result.get_outputs()\n",
    "        if isinstance(outs, list) and outs:\n",
    "            first = outs[0]\n",
    "            if hasattr(first, \"state\") and isinstance(first.state, ContentThreadState):\n",
    "                return first.state\n",
    "            # If first is a dict with stored thread namespace\n",
    "            if isinstance(first, dict):\n",
    "                ns = first.get(\"thread\") or first.get(\"state\")\n",
    "                if hasattr(ns, \"state\") and isinstance(ns.state, ContentThreadState):\n",
    "                    return ns.state\n",
    "\n",
    "    # Fallback: outs is a single namespace instead of list\n",
    "    if hasattr(result, \"get_outputs\"):\n",
    "        outs = result.get_outputs()\n",
    "        if hasattr(outs, \"state\") and isinstance(outs.state, ContentThreadState):\n",
    "            return outs.state\n",
    "\n",
    "    raise RuntimeError(\"Workflow did not return a ContentThreadState in .state or outputs[0].state\")\n",
    "\n",
    "def print_messages_from_state(state: ContentThreadState) -> None:\n",
    "    \"\"\"Print the message flow stored in `state.messages` in a compact form.\"\"\"\n",
    "    messages = getattr(state, \"messages\", None) or []\n",
    "    print(\"\\n--- Message Flow (ContentThreadState.messages) ---\")\n",
    "    for i, m in enumerate(messages):\n",
    "        role = m.get(\"role\", \"unknown\") if isinstance(m, dict) else str(type(m))\n",
    "        name = None\n",
    "        if isinstance(m, dict):\n",
    "            name = m.get(\"name\") or (m.get(\"metadata\") or {}).get(\"type\")\n",
    "        prefix = f\"{i:02d}: {role}\"\n",
    "        if name:\n",
    "            prefix += f\" ({name})\"\n",
    "        content_preview = str(m.get(\"content\", \"\")) if isinstance(m, dict) else \"\"\n",
    "        if len(content_preview) > 80:\n",
    "            content_preview = content_preview[:77] + \"...\"\n",
    "        print(f\"{prefix}: {content_preview}\")\n",
    "\n",
    "\n",
    "async def run_scenario(\n",
    "    topic: str,\n",
    "    brand: str,\n",
    "    brand_config: Dict[str, Any],\n",
    "    template: str = \"LINKEDIN_POST_ZERO_SHOT\",\n",
    "    examples: List[str] = [],\n",
    "    use_cot: bool = False,\n",
    "    max_iterations: int = 3,\n",
    "    quality_threshold: float = 7.0,\n",
    ") -> ContentThreadState:\n",
    "    \"\"\"Run the workflow and return the final ContentThreadState.\"\"\"\n",
    "    initial_message: Dict[str, Any] = {\n",
    "        \"brand\": brand,\n",
    "        \"topic\": topic,\n",
    "        \"brand_config\": brand_config,\n",
    "        \"template\": template,\n",
    "        \"examples\": examples,\n",
    "        \"quality_threshold\": quality_threshold,\n",
    "        \"use_cot\": use_cot,\n",
    "        \"max_iterations\": max_iterations,\n",
    "    }\n",
    "    result = await workflow.run(message=initial_message)\n",
    "    thread_state = _safe_get_thread_state_from_result(result)\n",
    "    return thread_state\n",
    "\n",
    "\n",
    "def show_result(state: ContentThreadState) -> None:\n",
    "    \"\"\"Render a human-readable summary of the final ContentThreadState.\"\"\"\n",
    "    print_messages_from_state(state)\n",
    "\n",
    "    draft = getattr(state, \"content\", None) or getattr(state, \"draft_content\", \"\")\n",
    "    print(\"\\n[bold green]Final Draft:[/bold green]\\n\")\n",
    "    rprint(draft)\n",
    "\n",
    "    print(\"\\n--- Evaluation ---\\n\")\n",
    "    critique = getattr(state, \"critique\", None) or getattr(state, \"evaluation\", None)\n",
    "    if critique:\n",
    "        score = getattr(critique, \"average_score\", None) or getattr(\n",
    "            critique, \"score\", None\n",
    "        )\n",
    "        meets = getattr(state, \"meets_quality_threshold\", None)\n",
    "        print(\n",
    "            f\"Score (weighted/average): {( '%.2f' % score) if isinstance(score, (int, float)) else 'N/A'} | \"\n",
    "            f\"Meets threshold: {meets}\"\n",
    "        )\n",
    "\n",
    "        scores_attr = getattr(critique, \"scores\", None)\n",
    "        if isinstance(scores_attr, dict):\n",
    "            print(\n",
    "                \"Dimension scores:\",\n",
    "                {k: f\"{v:.2f}\" for k, v in scores_attr.items() if isinstance(v, (int, float))},\n",
    "            )\n",
    "\n",
    "        reasoning = getattr(critique, \"reasoning\", None)\n",
    "        if reasoning:\n",
    "            print(\"Reasoning:\")\n",
    "            print(reasoning)\n",
    "\n",
    "        violations = getattr(critique, \"violations\", None) or []\n",
    "        if violations:\n",
    "            print(\"Violations:\")\n",
    "            for v in violations:\n",
    "                print(f\"- {v}\")\n",
    "    else:\n",
    "        print(\"No critique present (possibly ended before evaluation).\")\n",
    "\n",
    "    print(\"\\n--- Loop Stats ---\\n\")\n",
    "    iteration_count = getattr(state, \"iteration_count\", None)\n",
    "    max_iterations_val = getattr(state, \"max_iterations\", None)\n",
    "    quality_threshold_val = getattr(state, \"quality_threshold\", None)\n",
    "    print(\n",
    "        f\"iteration_count={iteration_count} | \"\n",
    "        f\"max_iterations={max_iterations_val} | \"\n",
    "        f\"quality_threshold={quality_threshold_val}\"\n",
    "    )\n",
    "\n",
    "    generation_metadata = getattr(state, \"generation_metadata\", None)\n",
    "    evaluation_metadata = getattr(state, \"evaluation_metadata\", None)\n",
    "    if generation_metadata:\n",
    "        print(\"\\nGeneration metadata:\")\n",
    "        rprint(generation_metadata)\n",
    "    if evaluation_metadata:\n",
    "        print(\"\\nEvaluation metadata:\")\n",
    "        rprint(evaluation_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80ae359",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Test 1 — RAG-only scenario (Internal brand content)\n",
    "\n",
    "Expected: content_planning routes to `rag_search`; tool executors run RAG; generation uses RAG context; evaluation runs; loop ends when threshold met or max iterations reached.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0694be",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "topic_rag = \"Create a post about our Public Cloud governance approach\"\n",
    "state_rag = await run_scenario(\n",
    "    brand=brand,\n",
    "    brand_config=brand_config,\n",
    "    topic=topic_rag,\n",
    "    template=\"LINKEDIN_POST_ZERO_SHOT\",\n",
    "    use_cot=True,\n",
    "    max_iterations=3,\n",
    "    quality_threshold=7.0,\n",
    ")\n",
    "show_result(state_rag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f346c9",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Test 2 — Web-only scenario (Current events)\n",
    "\n",
    "Expected: content_planning routes to `web_search`; generation uses web context.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab80ac0",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "topic_web = \"Create a post about the latest Public Cloud regulation news in 2025\"\n",
    "state_web = await run_scenario(\n",
    "    topic_web,\n",
    "    brand=brand,\n",
    "    brand_config=brand_config,\n",
    "    template=\"LINKEDIN_POST_ZERO_SHOT\",\n",
    "    use_cot=False,\n",
    "    max_iterations=3,\n",
    "    quality_threshold=7.0,\n",
    ")\n",
    "show_result(state_web)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0070b467",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Test 3 — Both tools scenario (Internal + Industry)\n",
    "\n",
    "Expected: content_planning routes to **both** `rag_search` and `web_search`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29210335",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "topic_both = \"Compare our Public Cloud governance approach to current industry standards in 2025\"\n",
    "state_both = await run_scenario(\n",
    "    topic_both,\n",
    "    brand=brand,\n",
    "    brand_config=brand_config,\n",
    "    template=\"LINKEDIN_POST_ZERO_SHOT\",\n",
    "    use_cot=True,\n",
    "    max_iterations=3,\n",
    "    quality_threshold=7.0,\n",
    ")\n",
    "show_result(state_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745cd871",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Test 4 — No-tools scenario (Opinion / Prompt-only)\n",
    "\n",
    "Expected: content_planning chooses no tools; generation runs directly from prompt and brand/template configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e940e02",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "topic_none = \"Share my opinion on why Public Cloud governance matters for small teams\"\n",
    "state_none = await run_scenario(\n",
    "    topic_none,\n",
    "    brand=brand,\n",
    "    brand_config=brand_config,\n",
    "    template=\"LINKEDIN_POST_ZERO_SHOT\",\n",
    "    use_cot=False,\n",
    "    max_iterations=3,\n",
    "    quality_threshold=7.0,\n",
    ")\n",
    "show_result(state_none)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550c0b87",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Optional Long-Form Scenario (Brand Journal-style Post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dea9e5",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "long_form_topic = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "state_long = await run_scenario(\n",
    "    long_form_topic,\n",
    "    brand=brand,\n",
    "    brand_config=brand_config,\n",
    "    template=\"BLOG_POST\",\n",
    "    use_cot=False,\n",
    "    max_iterations=3,\n",
    "    quality_threshold=7.0,\n",
    ")\n",
    "show_result(state_long)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa13c1c",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Summary\n",
    "\n",
    "- content_planning (Microsoft Agent Framework executor) routed correctly (RAG / Web / Both / None)  \n",
    "- tool executors populated research/tool contexts consumed by the generation executor  \n",
    "- content_evaluation executor returned a critique object, updated `iteration_count`, and set `meets_quality_threshold`  \n",
    "- Conditional routing (implemented inside the workflow graph) looped to regeneration when threshold was unmet and iterations remained  \n",
    "- Overall behavior should match the Week 4 LangGraph workflow, but using the Microsoft Agent Framework runtime and executors.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
