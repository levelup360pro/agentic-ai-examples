# Evaluation Rubric for Content Quality Assessment
# 5-Dimension Scoring System (1-10 scale)
# Used for both human manual scoring and AI-as-a-Judge evaluation

rubric_version: "1.0"
created_date: "2025-10-20"
methodology: "Evaluation-Driven Development (EDD)"

# Scoring Scale Definition
scale:
  min: 1
  max: 10
  passing_threshold: 7.0  # Content must score â‰¥7/10 average to pass quality gate
  excellence_threshold: 9.0  # Content scoring â‰¥9/10 considered excellent

# Dimension Definitions
dimensions:
  
  clarity:
    weight: 1.0
    description: "How clear and understandable is the content? Is it accessible to the target audience?"
    
    examples:
      score_3:
        description: "Confusing, jargon-heavy, unclear message"
        indicators:
          - "Multiple concepts jumbled together without clear structure"
          - "Heavy use of unexplained technical jargon or buzzwords"
          - "Reader cannot identify main takeaway after reading"
          - "Sentences are convoluted or overly complex"
        example_b2b: "Leveraging synergistic paradigms, our agentic orchestration facilitates multi-modal convergence through heterogeneous integration patterns."
        example_b2c: "Our revolutionary bioactive complexes synergize with dermal matrices to optimize cellular regeneration pathways through advanced molecular mechanisms."
      
      score_7:
        description: "Clear, accessible, logical flow"
        indicators:
          - "Single clear main message"
          - "Logical progression of ideas"
          - "Technical terms explained or avoided when appropriate"
          - "Reader can summarize key point after reading"
        example_b2b: "We tested three orchestration patterns. The evaluator-optimizer pattern delivered the best quality (7.8/10) at acceptable cost (â‚¬1.20/post)."
        example_b2c: "Our hyaluronic acid serum holds 1000x its weight in water, helping your skin stay hydrated all day. Apply after cleansing, before moisturizer."
      
      score_10:
        description: "Exceptionally clear, instantly understandable"
        indicators:
          - "Complex idea explained simply"
          - "Perfect structure (hook â†’ evidence/benefit â†’ takeaway)"
          - "Zero ambiguity"
          - "Reader immediately grasps concept and can explain to others"
        example_b2b: "95% of AI projects fail. We avoided that by defining 'good' before building. Here's the data."
        example_b2c: "Dry skin? Hyaluronic acid is your answer. It's like a water magnet for your skin. One drop holds 1000x its weight in moisture."

  brand_voice:
    weight: 1.0
    description: "Does the content match the brand's voice, tone, and style guidelines?"
    
    examples:
      score_3:
        description: "Generic, off-brand, could be anyone"
        indicators:
          - "No brand personality evident"
          - "Tone conflicts with brand guidelines"
          - "Vocabulary doesn't match brand style"
          - "Could be published by any competitor without changes"
        example_b2b: "ðŸš€ Revolutionizing AI! Our game-changing solution disrupts the paradigm! 10x results guaranteed! ðŸ”¥" (for evidence-led brand)
        example_b2c: "Our cutting-edge dermatological formulation leverages pharmaceutical-grade actives to optimize epidermal function." (for aspirational warm brand)
      
      score_7:
        description: "Recognizable brand tone, mostly consistent"
        indicators:
          - "Tone matches brand guidelines"
          - "Vocabulary aligns with brand style"
          - "Occasional generic phrases but overall on-brand"
          - "Reader familiar with brand would recognize it"
        example_b2b: "We tested LangGraph vs CrewAI on 15 pieces each. LangGraph won on observability (9/10 vs 7/10). Decision: LangGraph for production."
        example_b2c: "Transform your morning routine with our vitamin C serum. Bright, glowing skin starts here. Apply daily for radiant results."
      
      score_10:
        description: "Perfect brand alignment, unmistakable voice"
        indicators:
          - "Every sentence reflects brand personality"
          - "Vocabulary 100% aligned with brand style"
          - "Tone flawless throughout"
          - "Reader can identify brand from content alone"
        example_b2b: "Most AI content is theory. I'm running this system for my own brandsâ€”real posts, real engagement metrics, real costs tracked weekly. If the patterns aren't good enough for my reputation, they're not good enough for yours."
        example_b2c: "Your skin deserves transparency. Every ingredient, every benefit, every stepâ€”we share it all. Because self-care starts with knowing what you're putting on your skin. Nourish with intention."

  cta_strength:
    weight: 0.9
    description: "Is there a clear, compelling call-to-action? Does it drive the desired next step?"
    
    examples:
      score_3:
        description: "Weak, missing, or unclear CTA"
        indicators:
          - "No CTA present"
          - "Vague ask (e.g., 'Thoughts?', 'What do you think?')"
          - "Multiple conflicting CTAs"
          - "CTA doesn't match content context"
        example_b2b: "Interesting developments in AI. What do you think? Let me know your thoughts in the comments."
        example_b2c: "We love our new serum! Check out our website for more info. Also follow us on Instagram. And subscribe to our newsletter!"
      
      score_7:
        description: "Clear, relevant CTA"
        indicators:
          - "Single clear ask"
          - "CTA matches content context"
          - "Actionable (reader knows exactly what to do)"
          - "Relevant to audience needs"
        example_b2b: "If you're trying to get AI past compliance, follow along. I'm sharing builds, numbers, and patterns that scale. DM me for a 30-minute architecture audit."
        example_b2c: "Ready to try clean beauty? Shop our starter kitâ€”three essentials for your morning routine. Link in bio."
      
      score_10:
        description: "Compelling CTA with urgency and specificity"
        indicators:
          - "Clear, specific, actionable"
          - "Creates appropriate urgency without being pushy"
          - "Low friction (easy to act on)"
          - "Aligned with reader's immediate need"
        example_b2b: "Challenge the approach. Ask for the data. Or DM 'AUDIT' for a free 30-minute architecture review (2 slots left this week)."
        example_b2c: "Your best skin starts today. Try our vitamin C serum risk-freeâ€”30-day money-back guarantee. Use code GLOW20 for 20% off your first order."

  technical_accuracy:
    weight: 1.2
    description: "Are all facts, statistics, claims, and product information accurate and verifiable?"
    
    examples:
      score_3:
        description: "Factual errors, unverifiable claims, misleading information"
        indicators:
          - "Statistics without sources"
          - "Technical errors or outdated information"
          - "Exaggerated or unverifiable claims"
          - "Medical claims without proper disclaimers (B2C)"
        example_b2b: "Our AI achieves 99.9% accuracy with zero hallucinations. Studies show 100% of enterprises will adopt agentic AI by 2026."
        example_b2c: "Our serum cures acne in 24 hours! Clinically proven to eliminate wrinkles permanently! Dermatologists say it's better than Botox!"
      
      score_7:
        description: "Accurate, current, verifiable, appropriately qualified"
        indicators:
          - "Statistics cited with source or clearly marked as estimates"
          - "Technical/product details correct"
          - "Current information (not outdated)"
          - "Claims are realistic and verifiable"
          - "Proper disclaimers where needed"
        example_b2b: "95% of GenAI pilots fail to reach production (MIT 2025 study). We tested 4 model configs on 15 pieces each. GPT-4o-mini delivered 7.2/10 quality at â‚¬0.45/post."
        example_b2c: "Hyaluronic acid can hold up to 1000 times its weight in water, helping skin retain moisture. Our serum contains 2% pure hyaluronic acid. Results may vary by skin type."
      
      score_10:
        description: "Expert-level precision, fully sourced, appropriately nuanced"
        indicators:
          - "Every claim backed by data or credible source"
          - "Technical/product details precise and complete"
          - "Acknowledges limitations, not just benefits"
          - "Could be used as reference material"
        example_b2b: "text-embedding-3-small (1536D) outperforms ada-002 on MTEB benchmarks while costing less per token. We validated this with 40 test queries: retrieval precision 0.89 vs 0.82 (p<0.05)."
        example_b2c: "Our vitamin C serum uses L-ascorbic acid (15% concentration), the most researched form for brightening. Packaged in UV-protected glass to prevent oxidation. Patch test recommended for sensitive skin."

  engagement_potential:
    weight: 1.0
    description: "How likely is this content to generate engagement (likes, comments, shares, saves, profile views)?"
    
    examples:
      score_3:
        description: "No hook, low value, generic, forgettable"
        indicators:
          - "No compelling opening"
          - "No clear value for reader"
          - "Generic content seen everywhere"
          - "No reason to engage (comment, share, save)"
        example_b2b: "I'm excited to share that I've been working on an AI project. It's been a great learning experience. Looking forward to sharing more updates soon."
        example_b2c: "We're so excited about our products! We love clean beauty and hope you do too! Stay tuned for more updates about our brand journey!"
      
      score_7:
        description: "Good hook, clear value, shareable"
        indicators:
          - "Strong opening (surprising stat, bold claim, relatable problem)"
          - "Clear value for reader (actionable insight, useful information)"
          - "Specific enough to be interesting, not generic"
          - "Reader might comment, share, or save"
        example_b2b: "95% of GenAI pilots fail. Week 1: I defined 'good' before writing code. Quality â‰¥7/10, cost <â‚¬2/post, latency <60s. If a pattern can't hit these with data, it doesn't ship."
        example_b2c: "Dry, flaky skin in winter? Here's why: Cold air holds less moisture, stealing hydration from your skin. Fix it: Hyaluronic acid in the AM, rich moisturizer at night. Your skin will thank you."
      
      score_10:
        description: "Irresistible hook, high value, highly shareable, stops the scroll"
        indicators:
          - "Hook stops scroll (contrarian take, surprising data, relatable pain point)"
          - "High value density (multiple insights, immediately actionable)"
          - "Unique angle (transparent data, personal experience, fresh perspective)"
          - "Reader compelled to engage (save, share, DM, comment)"
        example_b2b: "Everyone talks AI. I'm running this system for my own brandsâ€”real posts, real engagement, real costs. Weekly. If I won't trust these patterns with my reputation, why would you trust them with compliance? Follow along. Challenge the approach. Ask for the data."
        example_b2c: "I spent â‚¬500 on serums that promised glowing skin. None worked. Then I learned the truth: most vitamin C serums oxidize before you even open them. Here's what actually works: L-ascorbic acid, 15% concentration, UV-protected packaging. I'll never buy another brown bottle again."

# Scoring Guidelines
scoring_guidelines:
  - "Score each dimension independently (1-10)"
  - "Use examples as anchors: 3 = poor, 7 = good, 10 = excellent"
  - "Half-point increments allowed (e.g., 7.5) for nuanced scoring"
  - "Calculate average: (clarity + brand_voice + cta_strength + technical_accuracy + engagement_potential) / 5"
  - "Apply weights if needed: weighted_avg = (clarity*1.0 + brand_voice*1.0 + cta*0.9 + accuracy*1.2 + engagement*1.0) / 5.1"
  - "Content must score â‰¥7.0 average to pass quality gate"
  - "Flag for human review if any dimension <5.0"
  - "Document reasoning for each score (especially <7 or >9)"

# Brand-Specific Adjustments
brand_adjustments:
  b2b_technical:
    # For B2B brands (e.g., AI consulting, SaaS, enterprise services)
    brand_voice_emphasis: "Direct, technical, evidence-led; avoid hype and buzzwords"
    technical_accuracy_critical: true  # Higher scrutiny on data, stats, technical claims
    engagement_notes: "Data-driven posts, transparent testing, and case studies typically perform best"
    example_brands: ["AI consulting", "Cloud architecture", "Enterprise SaaS"]
  
  b2c_aspirational:
    # For B2C brands (e.g., beauty, wellness, lifestyle)
    brand_voice_emphasis: "Aspirational, empowering, warm; focus on benefits and emotions"
    technical_accuracy_notes: "Avoid medical claims without disclaimers; explain ingredients simply; focus on benefits over mechanisms"
    engagement_notes: "Product education, customer stories, and self-care content typically perform best"
    example_brands: ["Clean beauty", "Wellness", "Sustainable lifestyle"]

# Quality Gates
quality_gates:
  generation:
    min_average: 7.0
    min_any_dimension: 5.0  # No dimension can score below 5
    action_if_fail: "Trigger optimization loop"
  
  hitl_approval:
    min_average: 7.0
    human_override: true  # Human can approve <7.0 with documented reason
    action_if_fail: "Reject or request revision with specific feedback"
  
  publication:
    min_average: 7.0
    all_dimensions_above: 6.0  # All dimensions must be â‰¥6.0
    action_if_fail: "Do not publish; return to generation"

# AI-as-a-Judge Calibration
ai_judge_calibration:
  target_correlation: 0.7  # Pearson correlation with human scores (minimum acceptable)
  target_mae: 1.0  # Mean absolute error per dimension (maximum acceptable)
  calibration_frequency: "Weekly during testing phase; monthly in production"
  calibration_sample_size: 20  # Minimum pieces for calibration
  notes: "If correlation <0.7 or MAE >1.0, adjust AI-judge prompts and re-calibrate"

# Evaluation Metadata
evaluation_metadata:
  evaluator_types:
    - "human"  # Manual scoring by content creator
    - "ai_judge"  # Automated scoring via LLM
  
  required_fields:
    - "content_id"
    - "evaluator"  # human or ai_judge
    - "timestamp"
    - "scores"  # Dict with 5 dimensions
    - "average"
    - "reasoning"  # Brief explanation for each score
  
  output_format: "CSV with columns: content_id, clarity, brand_voice, cta, accuracy, engagement, average, evaluator, timestamp, reasoning"

# Usage Instructions
usage:
  manual_scoring:
    - "Load rubric YAML"
    - "Read content piece"
    - "Score each dimension (1-10) using examples as anchors"
    - "Consider brand type (B2B technical vs B2C aspirational) when scoring brand_voice"
    - "Document reasoning for each score"
    - "Calculate average"
    - "Save to scores CSV"
  
  ai_judge_scoring:
    - "Load rubric YAML"
    - "Build evaluation prompt with rubric definitions and examples"
    - "Include brand-specific adjustments based on brand type"
    - "Call LLM (GPT-4o-mini or GPT-4o)"
    - "Parse response to extract 5 scores + reasoning"
    - "Calculate average"
    - "Save to scores CSV with evaluator='ai_judge'"
  
  calibration:
    - "Score same 20 pieces with both human and AI-judge"
    - "Calculate Pearson correlation per dimension"
    - "Calculate MAE per dimension"
    - "If correlation <0.7 or MAE >1.0, adjust AI-judge prompts"
    - "Re-calibrate weekly until stable"

# Notes
notes:
  - "This rubric is version 1.0; expect refinements based on testing and production use"
  - "Brand-specific adjustments allow single rubric to work across B2B and B2C brands"
  - "Weights can be adjusted per brand type (e.g., increase technical_accuracy weight for B2B)"